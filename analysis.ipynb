{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Raizen challenge - Data Analysis\n### by: Henrique Voni\n\n## Description\nThe challenge consists of forecasting the number of people that uses NYC metro using 7 years of history collected from turnstiles and stations. Most of the registries are audits that occurred every 4 hours, along with other entries (see `DESC` field description).\n\n\n## TL;DR Proposed solution\nThe solution is proposed in two parts: the first one contains all data visualization and exploratory data analysis that guided me in the modelling part. I'd rather focus on the 4 busiest stations since (a priori) they don't share any relevant information with each other, and therefore it was generated one model per station.\n\n### Modelling \nIt worth mentioning that i don't like to plug any DNN and expect good results without understanding what the problem requires. So, i like to start with simple/classic models and then proceed adding complexity to the solution. In this case, i used ARIMA models with a good analysis on parameter discovery and an ETS model (Exponential Smoothing). In a more depth analysis, i'd apply deep networks and/or Transformers that can capture long term temporal correlations in data.\n\n### Handling with data\nPerformance was a big part of this challenge. Some memory optimizations were made in order to work with the dataset, but `pandas` is not the best choice around for dealing with this amount of data. On a real case with appropriate time, i'd explore `dask` or `polars` for a better suitable solution.\n\nPlease consider that this notebook requires ~10Gb RAM.\n\n\n## Dataset Field Description\n\n- CA = Control Area (A002) \n- UNIT = Remote Unit for a station (R051) \n- SCP = Subunit Channel Position represents an specific address for a device (02-00-00) \n- STATION = Represents the station name the device is located at \n- LINENAME = Represents all train lines that can be boarded at this station Normally lines are represented by one character. LINENAME 456NQR repersents train server for 4, 5, 6, N, Q, and R trains. \n- DIVISION = Represents the Line originally the station belonged to BMT, IRT, or IND\n- TIME = Represents the datetime value for the registry\n- DESC = Represent the REGULAR scheduled audit event (Normally occurs every 4 hours) \n    1. Audits may occur more that 4 hours due to planning, or troubleshooting activities. \n    2. Additionally, there may be a RECOVR AUD entry This refers to a missed audit that was recovered. \n- ENTRIES = The comulative entry register value for a device \n- EXIST = The cumulative exit register value for a device\n\n\n### If you have any questions or issues running the experiment, i'd be more than happy to help at voni.henrique@gmail.com","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\nfrom sklearn.metrics import mean_squared_error as MSE\n\nfrom scipy.signal import detrend\n\nsns.set_theme()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-15T18:43:36.390580Z","iopub.execute_input":"2022-07-15T18:43:36.390976Z","iopub.status.idle":"2022-07-15T18:43:37.823435Z","shell.execute_reply.started":"2022-07-15T18:43:36.390879Z","shell.execute_reply":"2022-07-15T18:43:37.822372Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# FUNCTIONS \ndef downcast_columns(df):\n    for column in df:\n        if df[column].dtype == 'float64':\n            df[column]=pd.to_numeric(df[column], downcast='float')\n        if df[column].dtype == 'int64':\n            df[column]=pd.to_numeric(df[column], downcast='integer')\n    return df\n\n\ndef generate_train_test(df=None, station_name=None, test_date_start=None, detrendify=True):\n    station_df = df.iloc[df.index.get_level_values('station') == station_name]\n    \n    if detrendify:\n        station_df['detrended'] = detrend(station_df.traffic)\n\n    x_train = station_df.iloc[station_df.index.get_level_values('time') < test_date_start].reset_index().detrended.astype(float)\n    x_test = station_df.iloc[station_df.index.get_level_values('time') > test_date_start].reset_index().detrended.astype(float)\n    return x_train, x_test\n\ndef analyze_arima_params(x_train=None):\n    # parameter D\n    test = adfuller(x_train)\n    print('ADF Statistic: %f' % test[0])\n    print('p-value: %f' % test[1])\n\n    # parameter P\n    plot_pacf(x_train.diff().dropna())\n    plt.show()\n\n    # parameter Q\n    plot_acf(x_train.diff().dropna())\n    plt.show()\n\n\ndef model_pipeline(model_name='ARIMA', p=[1,5], d=0, q=[7,11], x_train=None, x_test=None):\n    history = [x for x in x_train]\n    predictions = []\n    \n    for t in range(len(x_test)):\n        if model_name == 'ARIMA':\n            model = ARIMA(history, order=(p, d, q))\n        else:\n            model = SimpleExpSmoothing(history)\n\n        model_fit = model.fit()\n        output = model_fit.forecast()\n        yhat = output[0]\n        predictions.append(yhat)\n        obs = x_test[t]\n        history.append(obs)\n    \n    return history, predictions","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:04:20.680570Z","iopub.execute_input":"2022-07-15T18:04:20.681169Z","iopub.status.idle":"2022-07-15T18:04:20.699141Z","shell.execute_reply.started":"2022-07-15T18:04:20.681131Z","shell.execute_reply":"2022-07-15T18:04:20.698191Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = '/kaggle/input/mta-dataset'\n\ncolumns = ['time', 'station', 'desc', 'entries', 'exits']\ncolumn_dtypes = {\n    'station' : 'string',\n    'desc' : 'string', \n    'entries' : 'Int32',\n    'exits' : 'Int32'\n}\n\ndf = pd.concat([pd.read_csv(file, \n                             dtype=column_dtypes, \n                             usecols=columns, \n                             parse_dates=['time']) \n                 for file \n                 in glob(f'{INPUT_PATH}/*.csv')])\n\ndf.info(memory_usage='deep')\n# for file in sorted(glob(f'{INPUT_PATH}/*.csv')):\n#     dfs.append(pd.read_csv(file, parse_dates=['time']).convert_dtypes())\n\n# df = pd.concat(dfs)\n\n# df = pd.read_csv('/kaggle/input/mta-dataset/2010.csv', parse_dates=['time']).convert_dtypes()\n# df.info(memory_usage = \"deep\")","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:04:20.700993Z","iopub.execute_input":"2022-07-15T18:04:20.701659Z","iopub.status.idle":"2022-07-15T18:10:37.742830Z","shell.execute_reply.started":"2022-07-15T18:04:20.701575Z","shell.execute_reply":"2022-07-15T18:10:37.741976Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = downcast_columns(df)","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:10:37.745758Z","iopub.execute_input":"2022-07-15T18:10:37.746192Z","iopub.status.idle":"2022-07-15T18:10:37.751180Z","shell.execute_reply.started":"2022-07-15T18:10:37.746154Z","shell.execute_reply":"2022-07-15T18:10:37.750495Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.desc.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:10:37.752168Z","iopub.execute_input":"2022-07-15T18:10:37.752524Z","iopub.status.idle":"2022-07-15T18:10:50.466971Z","shell.execute_reply.started":"2022-07-15T18:10:37.752493Z","shell.execute_reply":"2022-07-15T18:10:50.466061Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df = df[df.desc == \"REGULAR\"]\ndf = df.drop('desc', axis=1)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:10:50.468849Z","iopub.execute_input":"2022-07-15T18:10:50.469246Z","iopub.status.idle":"2022-07-15T18:11:11.873795Z","shell.execute_reply.started":"2022-07-15T18:10:50.469132Z","shell.execute_reply":"2022-07-15T18:11:11.872827Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:11:11.875562Z","iopub.execute_input":"2022-07-15T18:11:11.875921Z","iopub.status.idle":"2022-07-15T18:11:11.894998Z","shell.execute_reply.started":"2022-07-15T18:11:11.875879Z","shell.execute_reply":"2022-07-15T18:11:11.894055Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"by_station = df.groupby(['station', 'time'], as_index=False).agg({'entries': 'sum', 'exits':'sum'}).convert_dtypes()\ndel df","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:11:11.896417Z","iopub.execute_input":"2022-07-15T18:11:11.896675Z","iopub.status.idle":"2022-07-15T18:11:53.015658Z","shell.execute_reply.started":"2022-07-15T18:11:11.896641Z","shell.execute_reply":"2022-07-15T18:11:53.014957Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Feature cleaning and processing\n\nIt was noticed that `entries` and `exits` strangely appears to have negative values. Since these negative registries are quantitatively similar to the regular values, it was assumed that it is a typo and the absolute value was considered.\n\nAlso, there were registries with null values, meaning that a turnstile could be deffective or inoperant for a moment. I decided not to treat these isolated cases with interpolation because the sum of all turnstiles of a station would still be normal.","metadata":{}},{"cell_type":"code","source":"by_station[['entries','exits']] = by_station[['entries','exits']].apply(lambda x: abs(x))\nby_station['traffic'] = by_station['entries'] + by_station['exits']\nby_station['weekday'] = by_station['time'].dt.day_name()\nby_station['hour'] = by_station['time'].dt.hour","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:11:53.016976Z","iopub.execute_input":"2022-07-15T18:11:53.017381Z","iopub.status.idle":"2022-07-15T18:11:58.574495Z","shell.execute_reply.started":"2022-07-15T18:11:53.017330Z","shell.execute_reply":"2022-07-15T18:11:58.573844Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"by_station.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:11:58.577369Z","iopub.execute_input":"2022-07-15T18:11:58.577815Z","iopub.status.idle":"2022-07-15T18:11:58.595053Z","shell.execute_reply.started":"2022-07-15T18:11:58.577767Z","shell.execute_reply":"2022-07-15T18:11:58.594224Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"by_station.traffic.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:11:58.596519Z","iopub.execute_input":"2022-07-15T18:11:58.596792Z","iopub.status.idle":"2022-07-15T18:12:00.867389Z","shell.execute_reply.started":"2022-07-15T18:11:58.596761Z","shell.execute_reply":"2022-07-15T18:12:00.866640Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.ylim(0,1)\nsns.histplot(by_station.traffic, bins=10, kde=True, stat='probability')","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:12:00.868526Z","iopub.execute_input":"2022-07-15T18:12:00.868757Z","iopub.status.idle":"2022-07-15T18:12:55.048109Z","shell.execute_reply.started":"2022-07-15T18:12:00.868731Z","shell.execute_reply":"2022-07-15T18:12:55.047284Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"by_station[by_station.traffic > 1e10 * 0.2].shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:12:55.049373Z","iopub.execute_input":"2022-07-15T18:12:55.049647Z","iopub.status.idle":"2022-07-15T18:12:55.111994Z","shell.execute_reply.started":"2022-07-15T18:12:55.049596Z","shell.execute_reply":"2022-07-15T18:12:55.111063Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"by_station[by_station.traffic <= 1e10 * 0.2]","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:12:55.113486Z","iopub.execute_input":"2022-07-15T18:12:55.113712Z","iopub.status.idle":"2022-07-15T18:12:56.210778Z","shell.execute_reply.started":"2022-07-15T18:12:55.113686Z","shell.execute_reply":"2022-07-15T18:12:56.210000Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"by_station[by_station.traffic > 1e10 * 0.2].shape[0] / by_station.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:12:56.211818Z","iopub.execute_input":"2022-07-15T18:12:56.212033Z","iopub.status.idle":"2022-07-15T18:12:56.271685Z","shell.execute_reply.started":"2022-07-15T18:12:56.212008Z","shell.execute_reply":"2022-07-15T18:12:56.271111Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"a = by_station.set_index('time')\na","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:12:56.272565Z","iopub.execute_input":"2022-07-15T18:12:56.273249Z","iopub.status.idle":"2022-07-15T18:12:57.125713Z","shell.execute_reply.started":"2022-07-15T18:12:56.273214Z","shell.execute_reply":"2022-07-15T18:12:57.124876Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"traffic_by_station = by_station.groupby('station').sum().traffic","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:12:57.126874Z","iopub.execute_input":"2022-07-15T18:12:57.127084Z","iopub.status.idle":"2022-07-15T18:13:00.764900Z","shell.execute_reply.started":"2022-07-15T18:12:57.127059Z","shell.execute_reply":"2022-07-15T18:13:00.764002Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"traffic_by_station.sort_values(ascending=False).head(50)","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:13:00.766049Z","iopub.execute_input":"2022-07-15T18:13:00.766268Z","iopub.status.idle":"2022-07-15T18:13:00.774857Z","shell.execute_reply.started":"2022-07-15T18:13:00.766241Z","shell.execute_reply":"2022-07-15T18:13:00.774050Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"traffic_by_station.nlargest(4).index.values","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:13:00.776207Z","iopub.execute_input":"2022-07-15T18:13:00.776731Z","iopub.status.idle":"2022-07-15T18:13:00.790144Z","shell.execute_reply.started":"2022-07-15T18:13:00.776698Z","shell.execute_reply":"2022-07-15T18:13:00.789358Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# filter top 4 stations with most traffic\ntop_4_stations = traffic_by_station.nlargest(4).index.values\nb = a[a.station.isin(top_4_stations)]","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:13:00.792738Z","iopub.execute_input":"2022-07-15T18:13:00.793766Z","iopub.status.idle":"2022-07-15T18:13:01.349181Z","shell.execute_reply.started":"2022-07-15T18:13:00.793723Z","shell.execute_reply":"2022-07-15T18:13:01.348277Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"c = b.groupby(['station', 'weekday'], as_index=False).sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:13:01.350814Z","iopub.execute_input":"2022-07-15T18:13:01.351118Z","iopub.status.idle":"2022-07-15T18:13:01.412739Z","shell.execute_reply.started":"2022-07-15T18:13:01.351080Z","shell.execute_reply":"2022-07-15T18:13:01.412016Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing weekdays\n\nThe plot belows shows that business days represents the majority of volume/traffic. Curiously, the 2nd and 3rd busiest stations (25ST, CHAMBERS ST) present \"more constant\" values, with little gain in business days.","metadata":{}},{"cell_type":"code","source":"days = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\nstations = c.station.unique()\nplt.figure(figsize=(10,10))\nfor station in stations:\n    ax = c[c.station == station].set_index('weekday').reindex(days).traffic.plot(linewidth=3, fontsize=14, rot=45, ylabel='Traffic', grid=True)    \nax.legend(labels=stations, loc='best', bbox_to_anchor=(1,1))\nplt.title('Traffic per weekday', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:13:01.414078Z","iopub.execute_input":"2022-07-15T18:13:01.414808Z","iopub.status.idle":"2022-07-15T18:13:01.794769Z","shell.execute_reply.started":"2022-07-15T18:13:01.414771Z","shell.execute_reply":"2022-07-15T18:13:01.793886Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Monthly data\n\nSampling data in a monthly basis can help us detect some aspects such as trends, seasons, and other stuff that could be difficult to check in smaller different scales.\n\nFrom the plot below, we can verify different traffic rates during the year. However, a better analysis involves decomposing the series.","metadata":{}},{"cell_type":"code","source":"months = ['January', 'February', 'March',\n          'April', 'May', 'June', \n          'July', 'August', 'September', \n          'October', 'November', 'December']\n\n\nmonthly = b.groupby('station').resample('M').sum()\n\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nfor station, new_df in monthly.groupby(level=0, as_index=False):\n    x = list(range(1,13))\n    new_df = new_df.reset_index()\n    new_df['month_n'] = new_df.time.dt.month\n    volume_m = new_df.groupby('month_n').sum()\n    fig_axis = volume_m.plot(y='traffic', rot=45, ax=ax, linewidth=3, xlabel='Month', ylabel='Traffic')\n    fig_axis.set_xticks(x)\n    fig_axis.set_xticklabels(months)\n\n\nax.legend(stations)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:13:01.796040Z","iopub.execute_input":"2022-07-15T18:13:01.796248Z","iopub.status.idle":"2022-07-15T18:13:02.335507Z","shell.execute_reply.started":"2022-07-15T18:13:01.796223Z","shell.execute_reply":"2022-07-15T18:13:02.334715Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Decomposition analysis\n\nA time series can be decomposed in its base value, residuals, trend and seasonality. Decomposition comes handy when treating stationarity, detrending and other preprocessing techniques. Also, it can provide a good direction on model and parameter selection (such as ARIMA, which we will be using later).\n\n\nWe can see from the plots below that there's a clear trend in February with a drop in July-August. It is important to assert that our model will capture this behaviour during the forecasting task. ","metadata":{}},{"cell_type":"code","source":"for station, new_df in monthly.groupby(level=0, as_index=False):\n    print(f'Decomposition for {station} series')\n    x = list(range(0,12))\n    new_df = new_df.reset_index()\n    new_df['month_n'] = new_df.time.dt.month\n    volume_m = new_df.groupby('month_n').sum()\n    \n    plt.rcParams.update({'figure.figsize': (15,10)})\n    decompose_df = volume_m.reset_index().set_index('month_n')\n    result_mul = seasonal_decompose(decompose_df['traffic'], \n                                    model='additive', \n                                    period=1, \n                                    extrapolate_trend='freq')\n    result_mul.plot()\n    plt.show()    ","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:13:02.336765Z","iopub.execute_input":"2022-07-15T18:13:02.337078Z","iopub.status.idle":"2022-07-15T18:13:05.835120Z","shell.execute_reply.started":"2022-07-15T18:13:02.337043Z","shell.execute_reply":"2022-07-15T18:13:05.834285Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"hours = list(range(24))\n\nhourly = b.groupby(['station']).resample('H').sum().reset_index()\ntime = hourly.time\nhourly = hourly.groupby(['station', time.dt.hour]).sum()\n\nfig, ax = plt.subplots(figsize=(15,8))\nfor station, new_df in hourly.groupby(level=0, as_index=False):\n    print(new_df.traffic.shape)\n    fig_axis = new_df.plot(y='traffic', ax=ax, linewidth=1, linestyle='--', marker='o', xlabel='Hour', ylabel='Traffic', fontsize=16)\n    fig_axis.set_xticks(hours)\n    fig_axis.set_xticklabels(hours)\n    \nax.legend(stations)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:13:05.836379Z","iopub.execute_input":"2022-07-15T18:13:05.836634Z","iopub.status.idle":"2022-07-15T18:13:06.718301Z","shell.execute_reply.started":"2022-07-15T18:13:05.836585Z","shell.execute_reply":"2022-07-15T18:13:06.717741Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"There's a curious repetitive pattern along the hours. Highest values are found during business hours, which indicates people going/returning to/from work.","metadata":{}},{"cell_type":"code","source":"by_station.describe()","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:13:06.719427Z","iopub.execute_input":"2022-07-15T18:13:06.719772Z","iopub.status.idle":"2022-07-15T18:14:12.818514Z","shell.execute_reply.started":"2022-07-15T18:13:06.719733Z","shell.execute_reply":"2022-07-15T18:14:12.817598Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"daily = b.groupby('station').resample('D').sum()\ndaily\n\ndel by_station","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:14:12.819964Z","iopub.execute_input":"2022-07-15T18:14:12.820650Z","iopub.status.idle":"2022-07-15T18:14:13.065058Z","shell.execute_reply.started":"2022-07-15T18:14:12.820586Z","shell.execute_reply":"2022-07-15T18:14:13.063878Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"daily.describe()","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:14:13.068835Z","iopub.execute_input":"2022-07-15T18:14:13.069088Z","iopub.status.idle":"2022-07-15T18:14:13.128793Z","shell.execute_reply.started":"2022-07-15T18:14:13.069057Z","shell.execute_reply":"2022-07-15T18:14:13.127819Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Autocorrelation plot\n\nAutocorrelation plot shows that temporal correlation between the series and its lags are higher for lower lags/most recent previous values.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2,2, figsize=(10,8))\n\nfig.tight_layout(h_pad=4)\n\nfor ax, (station, new_df) in zip(axs.flatten(), daily.groupby(level=0, as_index=False)):\n    pd.plotting.autocorrelation_plot(new_df.traffic.tolist(), ax=ax)\n    ax.set_title(station)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:14:13.130304Z","iopub.execute_input":"2022-07-15T18:14:13.130695Z","iopub.status.idle":"2022-07-15T18:14:14.497400Z","shell.execute_reply.started":"2022-07-15T18:14:13.130654Z","shell.execute_reply":"2022-07-15T18:14:14.496654Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"top_4_stations","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:14:14.499038Z","iopub.execute_input":"2022-07-15T18:14:14.499520Z","iopub.status.idle":"2022-07-15T18:14:14.505806Z","shell.execute_reply.started":"2022-07-15T18:14:14.499476Z","shell.execute_reply":"2022-07-15T18:14:14.505014Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Modelling: forecasting 2017 data\n\nIn order to generate a model that can capture temporal patterns, i used 2010-2016 for model training and 2017 traffic as evaluation.\n\nPersonally, i like to start simple. There's no need to plug a fancy DNN and expect good results if simpler models are good options. In light of this, i proposed the usage of ARIMA and Exponential Smoothing methods. \n\nFor ARIMA, i made a basic analysis that could guide the choice of `[p, d, q]` parameters. For `d`, i applied `Adfuller`'s hypothesis test to check stationarity. `p` and `q` were analyzed with autocorrelation and partial correlation plots. Lag values superior to the threshold area should perform better. In a real case scenario with adequate computational power, hyperparameter selection techniques such as `Grid Search` / `Random Search` could be applied.\n\nI decided to create a model for each station, since i didn't make any correlation analysis among stations and/or used exogenous variables in training.\n\nAs an evaluation metric, i'm using Root Mean Squared Error (RMSE) that sums residuals and keep error in the same scale of input data.","metadata":{}},{"cell_type":"markdown","source":"# Predicting 23 ST series","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nx_train, x_test = generate_train_test(df=daily, station_name='23 ST', test_date_start='2017-01-01')\nanalyze_arima_params(x_train)\n\nhistory, predictions = model_pipeline(model_name='ARIMA', x_train=x_train, x_test=x_test, p=1, d=1, q=0)\n\nplt.plot(x_test)\nplt.plot(predictions, color='red')\nplt.legend(['actual', 'predicted'])\nplt.show()\n\n\nrmse = np.sqrt(MSE(x_test, predictions))\nprint(f'RMSE ARIMA: {rmse}')\n\n\nhistory, predictions = model_pipeline(model_name='ETS', x_train=x_train, x_test=x_test)\n\nplt.plot(x_test)\nplt.plot(predictions, color='red')\nplt.legend(['actual', 'predicted'])\nplt.show()\n\n\nrmse = np.sqrt(MSE(x_test, predictions))\nprint(f'RMSE ETS: {rmse}')","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:18:28.482937Z","iopub.execute_input":"2022-07-15T18:18:28.483195Z","iopub.status.idle":"2022-07-15T18:19:09.999249Z","shell.execute_reply.started":"2022-07-15T18:18:28.483165Z","shell.execute_reply":"2022-07-15T18:19:09.998352Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Predicting 125 ST series","metadata":{}},{"cell_type":"code","source":"x_train, x_test = generate_train_test(df=daily, station_name='125 ST', test_date_start='2017-01-01')\nanalyze_arima_params(x_train)\n\nhistory, predictions = model_pipeline(model_name='ARIMA', x_train=x_train, x_test=x_test, p=1, d=1, q=0)\n\nplt.plot(x_test)\nplt.plot(predictions, color='red')\nplt.legend(['actual', 'predicted'])\nplt.show()\n\n\nrmse = np.sqrt(MSE(x_test, predictions))\nprint(f'RMSE ARIMA: {rmse}')\n\n\nhistory, predictions = model_pipeline(model_name='ETS', x_train=x_train, x_test=x_test)\n\nplt.plot(x_test)\nplt.plot(predictions, color='red')\nplt.legend(['actual', 'predicted'])\nplt.show()\n\n\nrmse = np.sqrt(MSE(x_test, predictions))\nprint(f'RMSE ETS: {rmse}')","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:19:10.000787Z","iopub.execute_input":"2022-07-15T18:19:10.001019Z","iopub.status.idle":"2022-07-15T18:19:45.666714Z","shell.execute_reply.started":"2022-07-15T18:19:10.000993Z","shell.execute_reply":"2022-07-15T18:19:45.665905Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Predicting CHAMBERS ST series","metadata":{}},{"cell_type":"code","source":"x_train, x_test = generate_train_test(df=daily, station_name='CHAMBERS ST', test_date_start='2017-01-01')\nanalyze_arima_params(x_train)\n\nhistory, predictions = model_pipeline(model_name='ARIMA', x_train=x_train, x_test=x_test, p=1, d=1, q=0)\n\nplt.plot(x_test)\nplt.plot(predictions, color='red')\nplt.legend(['actual', 'predicted'])\nplt.show()\n\n\nrmse = np.sqrt(MSE(x_test, predictions))\nprint(f'RMSE ARIMA: {rmse}')\n\n\nhistory, predictions = model_pipeline(model_name='ETS', x_train=x_train, x_test=x_test)\n\nplt.plot(x_test)\nplt.plot(predictions, color='red')\nplt.legend(['actual', 'predicted'])\nplt.show()\n\n\nrmse = np.sqrt(MSE(x_test, predictions))\nprint(f'RMSE ETS: {rmse}')","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:19:45.669363Z","iopub.execute_input":"2022-07-15T18:19:45.669720Z","iopub.status.idle":"2022-07-15T18:20:20.578356Z","shell.execute_reply.started":"2022-07-15T18:19:45.669678Z","shell.execute_reply":"2022-07-15T18:20:20.577542Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Predicting FULTON ST series","metadata":{}},{"cell_type":"code","source":"x_train, x_test = generate_train_test(df=daily, station_name='FULTON ST', test_date_start='2017-01-01')\nanalyze_arima_params(x_train)\n\nhistory, predictions = model_pipeline(model_name='ARIMA', x_train=x_train, x_test=x_test, p=1, d=1, q=0)\n\nplt.plot(x_test)\nplt.plot(predictions, color='red')\nplt.legend(['actual', 'predicted'])\nplt.show()\n\n\nrmse = np.sqrt(MSE(x_test, predictions))\nprint(f'RMSE ARIMA: {rmse}')\n\n\nhistory, predictions = model_pipeline(model_name='ETS', x_train=x_train, x_test=x_test)\n\nplt.plot(x_test)\nplt.plot(predictions, color='red')\nplt.legend(['actual', 'predicted'])\nplt.show()\n\n\nrmse = np.sqrt(MSE(x_test, predictions))\nprint(f'RMSE ETS: {rmse}')","metadata":{"execution":{"iopub.status.busy":"2022-07-15T18:20:20.579513Z","iopub.execute_input":"2022-07-15T18:20:20.579773Z","iopub.status.idle":"2022-07-15T18:20:59.758135Z","shell.execute_reply.started":"2022-07-15T18:20:20.579743Z","shell.execute_reply":"2022-07-15T18:20:59.757100Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Model Insights\n\nWe can see clearly for every case that ARIMA models performed better on forecasting the series when compared to a simple smoothing method because it could identify patterns/trends better, despite of exponential smoothing generating lower RMSE metrics. Regarding the parameters of ARIMA, we can conclude that:\n\n- Adfuller hypothesis test shows that a p-value higher than 0.05 indicates the non-stationarity of the series in all cases. For sake of simplicity, i started setting ARIMA `d` parameter = `1` and got a good result. A further analysis should explore differencing techniques in order to make series stationary.\n\n- Partial Autocorrelation and Autocorrelation plots shows highest values at `0`, which means no correlation with lag values. However, good results were achieved with `p` and `q` parameters equal to 1;","metadata":{}},{"cell_type":"markdown","source":"# Insight summary and further work\n\n- MTA data has temporal nuances regarding traffic such as seasonality and trend in different sampling periods. \n- Weekend days are the less busiest days in stations;\n- Hourly sampling showed higher traffic in business hours, mostly caused by people going/returning to/from work.\n- Data indicates less usage of metro service in July-August and December-January. One possible cause is that July-August marks a vacation period (start of Summer) and December-January drop is related to Holidays.\n- ARIMA models could provide a decent forecasting using short lag periods (as indicated in autocorrelation analysis).;\n- Simpler models (ETS) couldn't converge and performed poorly on data.\n- Data is non-stationary and requires detrending;\n- Some registries pointed null values. Interpolation techniques should be applied in deeper investigations.\n- More advanced techniques (i.e. DNNs / Transformers) should capture long term correlations and temporal patterns. An interesting idea to go deeper in this analysis involves exploring time series imaging (check https://pyts.readthedocs.io/en/stable/modules/image.htmwith) techniques and CNNs.","metadata":{}}]}